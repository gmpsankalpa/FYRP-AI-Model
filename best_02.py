# -*- coding: utf-8 -*-
"""Best 02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MnTpu-jURZq_dZM415kwhC696DaWMfyv
"""

import pandas as pd
import random
from datetime import datetime, timedelta

def generate_dataset(rows=200000):
    data = []
    timestamp = datetime(2010, 6, 9, 0, 0, 0)

    for _ in range(rows):
        behavior = random.choices(['normal', 'theft', 'waste'], weights=[0.1, 0.4, 0.5])[0]
        voltage = round(random.uniform(228, 232), 2)

        if behavior == 'normal':
            current = round(random.uniform(1.4, 3.0), 2)
            power = round(voltage * current, 2)
            metered_power = power
            productive_power = power
            theft_flag = 0
            waste_flag = 0

        elif behavior == 'theft':
            current = round(random.uniform(3.2, 6.5), 2)
            power = round(voltage * current, 2)
            metered_power = round(power * random.uniform(0.5, 0.7), 2)  # Under-reported
            productive_power = metered_power
            theft_flag = 1
            waste_flag = 0

        elif behavior == 'waste':
            productive_power = round(random.uniform(200, 400), 2)
            current = round(random.uniform(2.5, 4.7), 2)
            power = round(voltage * current, 2)
            metered_power = power
            theft_flag = 0
            waste_flag = 1

        data.append({
            'Timestamp': timestamp.strftime('%Y-%m-%d %H:%M:%S'),
            'Voltage': voltage,
            'Current': current,
            'Power': power,
            'Metered_power': metered_power,
            'Productive_power': productive_power,
            'Theft': theft_flag,
            'Waste': waste_flag,
            'Label': behavior
        })

        timestamp += timedelta(minutes=1)

    return pd.DataFrame(data)

# Generate 10,000 rows of data
df = generate_dataset(200000)

# Show the first 5 rows
print(df.head())

# Save the dataset as a CSV file in Colab's environment
csv_path = "new_energy_dataset.csv"
df.to_csv(csv_path, index=False)
print(f"Dataset saved to {csv_path}")

# Install KerasTuner for hyperparameter optimization
!pip install -q keras-tuner

import pandas as pd
import numpy as np
import os
import joblib
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, mean_squared_error
import keras_tuner as kt
from sklearn.utils.class_weight import compute_class_weight
from google.colab import drive
import shutil

# --- Configuration ---
ORIGINAL_DATASET_FILENAME = 'sri_lanka_energy_dataset.csv'
NEW_DATASET_FILENAME = 'new_energy_dataset.csv'  # New CSV file
MODEL_DIR = 'models_optimized'
NEW_MODEL_DIR = 'models_optimized_v2'  # New directory for new models
DRIVE_MODEL_DIR = '/content/drive/MyDrive/Models_Optimized_V2'
os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(NEW_MODEL_DIR, exist_ok=True)

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)
os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)
print(f"Created Google Drive folder: {DRIVE_MODEL_DIR}")

# --- Step 1: Load and Combine Datasets ---
def load_and_validate_csv(filename):
    if not os.path.exists(filename):
        print(f"Error: Dataset file '{filename}' not found.")
        return None
    df = pd.read_csv(filename)
    required_columns = ["Voltage", "Current", "Power", "Theft", "Waste"]
    if not all(col in df.columns for col in required_columns):
        print(f"Error: File '{filename}' is missing required columns: {required_columns}")
        return None
    print(f"Dataset '{filename}' loaded successfully. Preview:")
    print(df.head())
    return df

# Load original dataset
df_original = load_and_validate_csv(os.path.join(DRIVE_MODEL_DIR,(ORIGINAL_DATASET_FILENAME)))
"""
df_original = load_and_validate_csv(ORIGINAL_DATASET_FILENAME)
"""
if df_original is None:
    exit()

# Load new dataset
df_new = load_and_validate_csv(NEW_DATASET_FILENAME)
if df_new is None:
    print("Continuing with original dataset only.")
    df_combined = df_original
else:
    # Combine datasets
    df_combined = pd.concat([df_original, df_new], ignore_index=True)
    print("\nCombined dataset preview:")
    print(df_combined.head())
    print(f"Total rows in combined dataset: {len(df_combined)}")

# Check for missing values
if df_combined.isnull().sum().any():
    print("Warning: Missing values detected. Filling with column means.")
    df_combined = df_combined.fillna(df_combined.mean(numeric_only=True))

# --- Step 2: Preprocessing and Scaling ---
FEATURE_NAMES = ["Voltage", "Current", "Power"]  # Define feature names for scaler
if "Timestamp" in df_combined.columns:
    df_combined = df_combined.drop(columns=["Timestamp"])

X = df_combined[FEATURE_NAMES]
y_theft = df_combined["Theft"]
y_waste = df_combined["Waste"]
y_power = df_combined["Power"]

# Check class balance
print("\nClass Distribution:")
print("Theft:\n", y_theft.value_counts())
print("Waste:\n", y_waste.value_counts())

# Compute class weights
class_weights_theft = compute_class_weight('balanced', classes=np.unique(y_theft), y=y_theft)
class_weights_theft = dict(enumerate(class_weights_theft))
class_weights_waste = compute_class_weight('balanced', classes=np.unique(y_waste), y=y_waste)
class_weights_waste = dict(enumerate(class_weights_waste))
print("\nClass Weights for Theft:", class_weights_theft)
print("Class Weights for Waste:", class_weights_waste)

# Fit scaler on combined data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
joblib.dump(scaler, os.path.join(NEW_MODEL_DIR, "feature_scaler_v2.pkl"))
print("\nScaler has been fitted and saved.")

# Data for classification and regression
X_clf_scaled = X_scaled
X_reg_scaled = X_scaled[:, :2]

# Split datasets
X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X_clf_scaled, y_theft, test_size=0.2, random_state=42, stratify=y_theft)
X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(X_clf_scaled, y_waste, test_size=0.2, random_state=42, stratify=y_waste)
X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(X_reg_scaled, y_power, test_size=0.2, random_state=42)

# --- Callbacks ---
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)

# --- Step 3: Model Training ---
def build_classification_model(hp):
    model = Sequential([
        Input(shape=(X_train_t.shape[1],)),
        Dense(units=hp.Int('units_1', min_value=32, max_value=128, step=32), use_bias=False),
        BatchNormalization(),
        LeakyReLU(negative_slope=0.1),
        Dropout(rate=hp.Float('dropout_1', min_value=0.2, max_value=0.4, step=0.1)),
        Dense(units=hp.Int('units_2', min_value=16, max_value=64, step=16)),
        LeakyReLU(negative_slope=0.1),
        Dense(1, activation='sigmoid')
    ])
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 5e-4, 1e-4])
    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss='binary_crossentropy', metrics=['accuracy'])
    return model

def build_regression_model(hp):
    model = Sequential([
        Input(shape=(X_train_p.shape[1],)),
        Dense(units=hp.Int('units_1', min_value=64, max_value=256, step=64)),
        LeakyReLU(negative_slope=0.1),
        Dropout(rate=hp.Float('dropout_1', min_value=0.2, max_value=0.4, step=0.1)),
        Dense(units=hp.Int('units_2', min_value=32, max_value=128, step=32)),
        LeakyReLU(negative_slope=0.1),
        Dense(1)
    ])
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 5e-4, 1e-4])
    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss=tf.keras.losses.MeanSquaredError(), metrics=['mse'])
    return model

# Option 1: Fine-tune existing models (uncomment to use)

print("\n--- Loading Existing Models for Fine-Tuning ---")
theft_model = load_model(os.path.join(DRIVE_MODEL_DIR, "theft_detection_model_v2.keras"))
waste_model = load_model(os.path.join(DRIVE_MODEL_DIR, "waste_detection_model_v2.keras"))
power_model = load_model(os.path.join(DRIVE_MODEL_DIR, "power_prediction_model_v2.keras"))

# Print model summaries
print("\nTheft Detection Model Summary:")
print(theft_model.summary())
print("\nWaste Detection Model Summary:")
print(waste_model.summary())
print("\nPower Prediction Model Summary:")
print(power_model.summary())

theft_model.fit(X_train_t, y_train_t, epochs=20, batch_size=32, validation_split=0.2, callbacks=[early_stop, reduce_lr], class_weight=class_weights_theft, verbose=1)
waste_model.fit(X_train_w, y_train_w, epochs=20, batch_size=32, validation_split=0.2, callbacks=[early_stop, reduce_lr], class_weight=class_weights_waste, verbose=1)
power_model.fit(X_train_p, y_train_p, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stop, reduce_lr], verbose=1)

# Evaluate models
theft_preds = (theft_model.predict(X_test_t, verbose=0) > 0.5).astype(int).flatten()
print("\nTheft Detection Report:")
print(classification_report(y_test_t, theft_preds))

waste_preds = (waste_model.predict(X_test_w, verbose=0) > 0.5).astype(int).flatten()
print("\nWaste Detection Report:")
print(classification_report(y_test_w, waste_preds))

power_preds = power_model.predict(X_test_p, verbose=0).flatten()
mse = mean_squared_error(y_test_p, power_preds)
rmse = np.sqrt(mse)
print(f"\nPower Prediction Mean Squared Error: {mse:.2f}")
print(f"Power Prediction Root Mean Squared Error: {rmse:.2f}")

# Option 2: Train new models from scratch
"""
print("\n--- Tuning and Training Theft Detection Model ---")
tuner_theft = kt.Hyperband(build_classification_model, objective='val_accuracy', max_epochs=30, factor=3, directory='keras_tuner', project_name='theft_detection_v2')
tuner_theft.search(X_train_t, y_train_t, epochs=30, validation_split=0.2, callbacks=[early_stop, reduce_lr], class_weight=class_weights_theft)
best_hps_theft = tuner_theft.get_best_hyperparameters(num_trials=1)[0]
theft_model = tuner_theft.hypermodel.build(best_hps_theft)
theft_model.fit(X_train_t, y_train_t, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stop, reduce_lr], class_weight=class_weights_theft, verbose=1)
theft_preds = (theft_model.predict(X_test_t) > 0.5).astype(int).flatten()
print("\nTheft Detection Report:")
print(classification_report(y_test_t, theft_preds))

print("\n--- Tuning and Training Waste Detection Model ---")
tuner_waste = kt.Hyperband(build_classification_model, objective='val_accuracy', max_epochs=30, factor=3, directory='keras_tuner', project_name='waste_detection_v2')
tuner_waste.search(X_train_w, y_train_w, epochs=30, validation_split=0.2, callbacks=[early_stop, reduce_lr], class_weight=class_weights_waste)
best_hps_waste = tuner_waste.get_best_hyperparameters(num_trials=1)[0]
waste_model = tuner_waste.hypermodel.build(best_hps_waste)
waste_model.fit(X_train_w, y_train_w, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stop, reduce_lr], class_weight=class_weights_waste, verbose=1)
waste_preds = (waste_model.predict(X_test_w) > 0.5).astype(int).flatten()
print("\nWaste Detection Report:")
print(classification_report(y_test_w, waste_preds))

print("\n--- Tuning and Training Power Prediction Model ---")
tuner_power = kt.Hyperband(build_regression_model, objective='val_loss', max_epochs=50, factor=3, directory='keras_tuner', project_name='power_prediction_v2')
tuner_power.search(X_train_p, y_train_p, epochs=50, validation_split=0.2, callbacks=[early_stop, reduce_lr])
best_hps_power = tuner_power.get_best_hyperparameters(num_trials=1)[0]
power_model = tuner_power.hypermodel.build(best_hps_power)
power_model.fit(X_train_p, y_train_p, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stop, reduce_lr], verbose=1)
power_preds = power_model.predict(X_test_p).flatten()
mse = mean_squared_error(y_test_p, power_preds)
rmse = np.sqrt(mse)
print(f"\nPower Prediction Mean Squared Error: {mse:.2f}")
print(f"Power Prediction Root Mean Squared Error: {rmse:.2f}")
"""

# --- Step 4: Save New Models ---
theft_model.save(os.path.join(NEW_MODEL_DIR, "theft_detection_model_v2.keras"))
waste_model.save(os.path.join(NEW_MODEL_DIR, "waste_detection_model_v2.keras"))
power_model.save(os.path.join(NEW_MODEL_DIR, "power_prediction_model_v2.keras"))
print(f"\nNew models and scaler saved in the '{NEW_MODEL_DIR}' directory.")

# --- Step 6: Save New Models to Google Drive ---
theft_model.save(os.path.join(DRIVE_MODEL_DIR, "theft_detection_model_v2.keras"))
waste_model.save(os.path.join(DRIVE_MODEL_DIR, "waste_detection_model_v2.keras"))
power_model.save(os.path.join(DRIVE_MODEL_DIR, "power_prediction_model_v2.keras"))
shutil.copy(os.path.join(NEW_MODEL_DIR, "feature_scaler_v2.pkl"), os.path.join(DRIVE_MODEL_DIR, "feature_scaler_v2.pkl"))
print(f"\nNew models and scaler saved in Google Drive at: {DRIVE_MODEL_DIR}")

# --- Step 7: Inference Example ---
print("\n--- Inference Example with New Models ---")
"""
loaded_scaler = joblib.load(os.path.join(NEW_MODEL_DIR, "feature_scaler_v2.pkl"))
loaded_theft_model = load_model(os.path.join(NEW_MODEL_DIR, "theft_detection_model_v2.keras"))
"""
loaded_scaler = joblib.load(os.path.join(DRIVE_MODEL_DIR, "feature_scaler_v2.pkl"))
loaded_theft_model = load_model(os.path.join(DRIVE_MODEL_DIR, "theft_detection_model_v2.keras"))

# Convert sample to DataFrame to avoid StandardScaler warning
sample = pd.DataFrame([[235, 1.5, 350]], columns=FEATURE_NAMES)
scaled_sample = loaded_scaler.transform(sample)

theft_prediction_prob = loaded_theft_model.predict(scaled_sample)
theft_status = "Theft Detected" if theft_prediction_prob[0][0] > 0.5 else "Normal"
print(f"Sample Input: {sample.values}")
print(f"Theft Probability: {theft_prediction_prob[0][0]:.4f} ({theft_status})")

"""
loaded_power_model = load_model(os.path.join(NEW_MODEL_DIR, "power_prediction_model_v2.keras"))
"""
loaded_power_model = load_model(os.path.join(DRIVE_MODEL_DIR, "power_prediction_model_v2.keras"))
sample_for_power = scaled_sample[:, :2]
predicted_power = loaded_power_model.predict(sample_for_power)
print(f"Predicted Power based on {sample[['Voltage', 'Current']].values[0]}: {predicted_power[0][0]:.2f} W")

import pandas as pd
import numpy as np
import os
import joblib
import tensorflow as tf
from tensorflow.keras.models import load_model
from sklearn.preprocessing import StandardScaler
from google.colab import drive

# --- Configuration ---
DRIVE_MODEL_DIR = '/content/drive/MyDrive/Models_Optimized_V2'
FEATURE_NAMES = ["Voltage", "Current", "Power"]

# --- Step 1: Mount Google Drive ---
drive.mount('/content/drive', force_remount=True)
print(f"Accessing Google Drive folder: {DRIVE_MODEL_DIR}")

# --- Step 2: Load Scaler and Models ---
def load_scaler_and_models():
    try:
        scaler = joblib.load(os.path.join(DRIVE_MODEL_DIR, "feature_scaler_v2.pkl"))
        theft_model = load_model(os.path.join(DRIVE_MODEL_DIR, "theft_detection_model_v2.keras"))
        waste_model = load_model(os.path.join(DRIVE_MODEL_DIR, "waste_detection_model_v2.keras"))
        power_model = load_model(os.path.join(DRIVE_MODEL_DIR, "power_prediction_model_v2.keras"))
        print("Scaler and models loaded successfully.")
        return scaler, theft_model, waste_model, power_model
    except Exception as e:
        print(f"Error loading scaler or models: {e}")
        return None, None, None, None

scaler, theft_model, waste_model, power_model = load_scaler_and_models()
if scaler is None or any(model is None for model in [theft_model, waste_model, power_model]):
    print("Failed to load scaler or models. Exiting.")
    exit()

# --- Step 3: Function to Get User Input ---
def get_user_input():
    try:
        print("\nEnter input parameters (or press Enter to use default sample: Voltage=235, Current=1.5, Power=350):")
        voltage = input("Voltage (V): ").strip()
        current = input("Current (A): ").strip()
        power = input("Power (W): ").strip()

        # Use default sample if no input provided
        if not voltage and not current and not power:
            print("Using default sample: Voltage=235, Current=1.5, Power=350")
            return pd.DataFrame([[235, 1.5, 350]], columns=FEATURE_NAMES)

        # Validate and convert inputs
        voltage = float(voltage)
        current = float(current)
        power = float(power)
        return pd.DataFrame([[voltage, current, power]], columns=FEATURE_NAMES)
    except ValueError as e:
        print(f"Invalid input. Please enter numeric values. Error: {e}")
        return None

# --- Step 4: Function to Make Predictions ---
def make_predictions(sample_df, scaler, theft_model, waste_model, power_model):
    try:
        # Scale the input sample
        scaled_sample = scaler.transform(sample_df)
        scaled_sample_for_power = scaled_sample[:, :2]  # Power model uses only Voltage and Current

        # Theft Detection Prediction
        theft_prob = theft_model.predict(scaled_sample, verbose=0)[0][0]
        theft_status = "Theft Detected" if theft_prob > 0.5 else "Normal"

        # Waste Detection Prediction
        waste_prob = waste_model.predict(scaled_sample, verbose=0)[0][0]
        waste_status = "Waste Detected" if waste_prob > 0.5 else "Normal"

        # Power Prediction
        predicted_power = power_model.predict(scaled_sample_for_power, verbose=0)[0][0]

        # Display Results
        print("\n--- Prediction Results ---")
        print(f"Input Parameters: Voltage={sample_df['Voltage'].values[0]} V, "
              f"Current={sample_df['Current'].values[0]} A, Power={sample_df['Power'].values[0]} W")
        print(f"Theft Prediction: Probability={theft_prob:.4f} ({theft_status})")
        print(f"Waste Prediction: Probability={waste_prob:.4f} ({waste_status})")
        print(f"Power Prediction: {predicted_power:.2f} W")
    except Exception as e:
        print(f"Error during prediction: {e}")

# --- Step 5: Main Execution ---
while True:
    sample_df = get_user_input()
    if sample_df is None:
        print("Try again with valid inputs.")
        continue

    make_predictions(sample_df, scaler, theft_model, waste_model, power_model)

    # Ask if user wants to test another sample
    retry = input("\nDo you want to test another sample? (yes/no): ").strip().lower()
    if retry != 'yes':
        print("Exiting.")
        break

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d ziya07/smart-grid-real-time-load-monitoring-dataset
!kaggle datasets download -d ziya07/powergridsense-dataset
!kaggle datasets download -d plegmalabs/handful-project-energy-data
!kaggle datasets download -d ziya07/smart-grid-stability-and-reliability-dataset
!kaggle datasets download -d ziya07/iot-enabled-smart-grid-dataset

import zipfile
zipfile.ZipFile("handful-project-energy-data.zip", "r").extractall("dataset")
zipfile.ZipFile("iot-enabled-smart-grid-dataset.zip", "r").extractall("dataset")
zipfile.ZipFile("powergridsense-dataset.zip", "r").extractall("dataset")
zipfile.ZipFile("smart-grid-real-time-load-monitoring-dataset.zip", "r").extractall("dataset")
zipfile.ZipFile("smart-grid-stability-and-reliability-dataset.zip", "r").extractall("dataset")